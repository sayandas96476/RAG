{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxYgYCg09hzuWm6RC3vqMV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayandas96476/RAG/blob/main/Agentic_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An agentic RAG  is created using GroqApi\n",
        "\n",
        "Data: Fetched from Wikipedia -> parsed using BeautifulSoup\n",
        "\n",
        "RAG: For RAG sparse retrieval TF-IDF is used.\n",
        "\n",
        "LLM model used: \"mixtral-8x7b-32768\"\n",
        "\n",
        "Tools created:\n",
        "\n",
        "    Entry Tool: For checking whether the query asked is related to the topic\n",
        "\n",
        "    RAG Tool: For retrieving information from document\n",
        "\n",
        "    Search Tool: If any information is not present in the document the search tool is called. DuckDuckGoSearch is used.\n",
        "\n"
      ],
      "metadata": {
        "id": "an9tJu3ORzcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Architechture:\n",
        "\n",
        "Document fetched from wikipedia, parsed using BeautifulSoup.\n",
        "\n",
        "Entry Tool: Checks if query is related to document.\n",
        "\n",
        "    If yes RAG or Search Tool is triggered\n",
        "    Else all operations are stopped and returned \"This is outside the topic\"\n",
        "\n",
        "RAG tool: RAG tool is developed using sparse retrieval\n",
        "\n",
        "Search Tool: If data not present in document base then Search Tool is triggered."
      ],
      "metadata": {
        "id": "-Bn4JQgiT_2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun  # Changed from Tavily\n",
        "from langchain_core.tools import tool"
      ],
      "metadata": {
        "id": "CdqzpAU4AmRv"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUk7V31-BCEf",
        "outputId": "4f694605-a6c1-4b23-de06-56dc3676e51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/3.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.3 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ddg_search = DuckDuckGoSearchRun()"
      ],
      "metadata": {
        "id": "LCigDbqjAx-x"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-groq langchain\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VmZy_WjA2_o",
        "outputId": "14d17c7e-2a38-4d39-ef0e-51edf04d9a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"  # Replace with your actual API key\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RwOhQ30BYd4",
        "outputId": "fe61d048-3543-445f-dfb3-8795d1abc210"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_full_wikipedia_content(title):\n",
        "    # Construct the URL for the full Wikipedia page\n",
        "    url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
        "\n",
        "    try:\n",
        "        # Make the request\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the main content div\n",
        "        content_div = soup.find(id=\"mw-content-text\")\n",
        "\n",
        "        # Extract all paragraphs\n",
        "        paragraphs = content_div.find_all('p')\n",
        "\n",
        "        # Combine all paragraph texts\n",
        "        full_text = '\\n\\n'.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        # Remove citations [1], [2], etc.\n",
        "        import re\n",
        "        full_text = re.sub(r'\\[\\d+\\]', '', full_text)\n",
        "\n",
        "        return full_text.strip()\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching page: {str(e)}\"\n",
        "\n",
        "\n",
        "def combine_strings(original_list, chunk_size=3):\n",
        "    return [''.join(original_list[i:i + chunk_size])\n",
        "            for i in range(0, len(original_list), chunk_size)]\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n"
      ],
      "metadata": {
        "id": "cA74j0m-BaT8"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"Batman\"\n",
        "text = get_full_wikipedia_content(title)\n",
        "text = text.replace('\\n', '')\n",
        "\n",
        "lis = text.split(\".\")\n",
        "\n",
        "combined = combine_strings(lis)\n",
        "text = \"\"\" \"\"\"\n",
        "for i in combined:\n",
        "  text += i+\"\\n\\n\\n\"\n",
        "\n",
        "documents = text.split(\"\\n\\n\\n\")\n",
        "len(documents)\n",
        "\n",
        "preprocessed_docs = [preprocess_text(doc) for doc in documents]"
      ],
      "metadata": {
        "id": "ixsxTIOsCN6I"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a ChatGroq instance\n",
        "llm = ChatGroq(\n",
        "    model_name=\"mixtral-8x7b-32768\",  # You can also use \"mixtral-8x7b-32768\"\n",
        "    temperature=0.1,\n",
        "    max_tokens=1024\n",
        ")"
      ],
      "metadata": {
        "id": "TO3OPhyACO6T"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context(preprocessed_docs,query):\n",
        "\n",
        "  # Step 2: Initialize the TfidfVectorizer\n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # Step 3: Fit and transform the documents to generate the TF-IDF matrix\n",
        "  tfidf_matrix = vectorizer.fit_transform(preprocessed_docs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Step 5: Transform the query into a TF-IDF vector\n",
        "  query_vector = vectorizer.transform([query])\n",
        "\n",
        "  # Step 6: Compute cosine similarity between the query and the documents\n",
        "  cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "\n",
        "  # Step 7: Get the top 3 results based on similarity scores\n",
        "  top_n = 10\n",
        "  top_indices = np.argsort(cosine_similarities)[::-1][:top_n]\n",
        "\n",
        "  CONTEXT = \"\"\"\"\"\"\n",
        "\n",
        "  for i, idx in enumerate(top_indices):\n",
        "      CONTEXT+= documents[idx] + \"\\n\"+ \"================\"+\"\\n\"\n",
        "  return CONTEXT"
      ],
      "metadata": {
        "id": "y2zKf77KCZBx"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ChatGroq instance\n",
        "model = ChatGroq(\n",
        "    model_name=\"mixtral-8x7b-32768\",  # You can also use \"mixtral-8x7b-32768\"\n",
        "    temperature=0.1,\n",
        "    max_tokens=1024\n",
        ")"
      ],
      "metadata": {
        "id": "5foLEsUHle77"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def rag(query):\n",
        "\n",
        "  #preprocessed_docs=preprocessed_docs.split(',')\n",
        "  CONTEXT = get_context(preprocessed_docs,query)\n",
        "  template=prompt = f\"\"\"\n",
        "    Use the following CONTEXT to answer the QUESTION at the end.\n",
        "    If you don't know the answer, just say that \"PROCEED_TO_SEARCH\", don't try to make up an answer.\n",
        "    Also remember don't use any external knowledge and only refer to this CONTEXT to answer question.\n",
        "    Consider this CONTEXT as the ultimate truth.\n",
        "\n",
        "\n",
        "  CONTEXT: {CONTEXT}\n",
        "  QUESTION: {query}\n",
        "\n",
        "  \"\"\"\n",
        "  prompt = PromptTemplate(\n",
        "      input_variables=[\"query\",\"CONTEXT\"],\n",
        "      template=template\n",
        "  )\n",
        "\n",
        "  chain = LLMChain(llm=llm, prompt=prompt)\n",
        "  response = chain.invoke({\"query\": query, \"CONTEXT\":CONTEXT})\n",
        "  return response['text']"
      ],
      "metadata": {
        "id": "amuQ6YmRDGbr"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def entry(query):\n",
        "\n",
        "  template=prompt = f\"\"\"\n",
        "    You will be provided a query.\n",
        "    Your task is to determine if this question is only related to \"Batman\".\n",
        "    DONT CONSIDER ANYTHING OUTSIDE THIS TOPIC.\n",
        "\n",
        "    If yes, return: \"PROCEED_TO_SEARCH\"\n",
        "    If no, return: \"This is outside the topic of Batman\"\n",
        "\n",
        "    Query: {query}\n",
        "    \"\"\"\n",
        "  prompt = PromptTemplate(\n",
        "      input_variables=[\"query\"],\n",
        "      template=template\n",
        "  )\n",
        "\n",
        "  chain = LLMChain(llm=llm, prompt=prompt)\n",
        "  response = chain.invoke({\"query\": query})\n",
        "  return response['text']\n",
        "\n",
        "query = \"Which is the highest IMDB rating movie till date?\"\n",
        "s = entry(query)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaJkqqiQk-7y",
        "outputId": "ac77b905-5e3c-4a87-987d-f9defedec974"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is outside the topic of Batman. The query is asking about the highest IMDB rated movie in general, not specifically related to Batman.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import Tool\n",
        "\n",
        "tools = [\n",
        "    Tool(name=\"Entry Tool\",\n",
        "          func=entry,\n",
        "          description=\"First checks if the query is strictly related to Batman. Use this tool first.\"),\n",
        "    Tool(name=\"Rag Tool\",\n",
        "          func=rag,\n",
        "          description=\"Use this tool after Entry Tool confirms Batman relevance. Searches local Batman knowledge base.\"),\n",
        "    Tool(name=\"Search Tool\",\n",
        "          func=ddg_search.run,\n",
        "          description=\"Use this as last resort if Rag Tool doesn't find the answer.\")\n",
        "]\n",
        "# Bind tools to the model\n",
        "llm.bind_tools(tools)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By_SmEMHqWgR",
        "outputId": "078c97fc-217c-484a-ae3f-d42e7825f9ad"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7bb62ba51e10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7bb6299020b0>, temperature=0.1, model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=1024), kwargs={'tools': [{'type': 'function', 'function': {'name': 'Entry Tool', 'description': 'First checks if the query is strictly related to Batman. Use this tool first.', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'Rag Tool', 'description': 'Use this tool after Entry Tool confirms Batman relevance. Searches local Batman knowledge base.', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'Search Tool', 'description': \"Use this as last resort if Rag Tool doesn't find the answer.\", 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}]}, config={}, config_factories=[])"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def final_answers(answer):\n",
        "\n",
        "  # Create a ChatGroq instance\n",
        "  model = ChatGroq(\n",
        "      model_name=\"mixtral-8x7b-32768\",  # You can also use \"mixtral-8x7b-32768\"\n",
        "      temperature=0.1,\n",
        "      max_tokens=1024\n",
        "  )\n",
        "  template=prompt = f\"\"\"\n",
        "  You will be used in the final step.\n",
        "  You will recieve input as response from Rag tool or search tool.\n",
        "  Your task is to remove the search steps.\n",
        "  Dont provide any unnecessary information not related to the query.\n",
        "\n",
        "  Give only the answer to the query.\n",
        "\n",
        "\n",
        "\n",
        "  Rule: Strictly follow the Example to produce the output\n",
        "  Example:\n",
        "  Input:\n",
        "    I will start by using my rag tool to find out who created Batman.\n",
        "\n",
        "    (After checking the rag tool)\n",
        "\n",
        "    I found some information that Batman was created by Bob Kane and Bill Finger.\n",
        "\n",
        "    (Passing the output to the final\\_answer tool)\n",
        "\n",
        "    The final answer is: Batman was created by Bob Kane and Bill Finger.\n",
        "\n",
        "  Output:\n",
        "    Batman was created by Bob Kane and Bill Finger.\n",
        "\n",
        "  prev_answer: {answer}\n",
        "\n",
        "  \"\"\"\n",
        "  prompt = PromptTemplate(\n",
        "      input_variables=[\"answer\"],\n",
        "      template=template\n",
        "  )\n",
        "\n",
        "  chain = LLMChain(llm=model, prompt=prompt)\n",
        "  response = chain.invoke({\"answer\": answer})\n",
        "  return response['text']\n"
      ],
      "metadata": {
        "id": "uj3Hj1gskMZO"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FINAL(query):\n",
        "\n",
        "  prompt = \"\"\"\n",
        "  You are a helpful assistant specializing in Batman-related information.\n",
        "  Follow these steps when answering:\n",
        "  1. First use the Entry Tool to check if the query is Batman-related\n",
        "  2. If Batman-related, use the Rag Tool to search local knowledge\n",
        "  3. If Rag Tool returns \"PROCEED_TO_SEARCH\", use the Search Tool\n",
        "  4. If at any point you get \"This is outside the topic\", stop and return that message\n",
        "\n",
        "  Think through each step carefully and use the appropriate tool.\n",
        "  Explain your thinking process.\n",
        "\n",
        "  Question: {query}\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  prompt+=query\n",
        "\n",
        "  # Generate a response\n",
        "  response = llm.predict(prompt)\n",
        "  #print(response)\n",
        "  #print(\"=\"*25)\n",
        "  response = final_answers(response)\n",
        "  return response"
      ],
      "metadata": {
        "id": "rTBie-R-G0b_"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Give me a list of Batman's villains?\"\n",
        "response = FINAL(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASE8KO1CsiA7",
        "outputId": "5ffc0ffa-bcd6-4ecf-be38-fa9c5f6bff99"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are some of Batman's most famous villains: 1. The Joker, 2. Catwoman, 3. The Penguin, 4. The Riddler, 5. Two-Face, 6. Poison Ivy, 7. Bane, 8. Scarecrow, 9. Mr. Freeze, and 10. Harley Quinn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"In which year was the first Batman movie released?\"\n",
        "response = FINAL(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4ruTNZftSef",
        "outputId": "69a6d191-1a89-4ddc-9051-08a55867e5f2"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first Batman movie was released in 1989.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Did any Batman movie recieved Oscars?\"\n",
        "response = FINAL(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFvfWgm7tcCb",
        "outputId": "66159ee5-70cb-421b-bf10-b999020ac94f"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Dark Knight (2008) received two Oscars for Best Sound Editing and Best Supporting Actor (Heath Ledger).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is the present Batman hero name in DCU film ?\"\n",
        "response = FINAL(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_wA_LMARQgZ",
        "outputId": "0d766334-2732-40fa-a4fb-70bbfb2f6726"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The present Batman hero name in the DCU film is Bruce Wayne, played by Ben Affleck. However, a new Batman, played by Robert Pattinson, will be introduced in the upcoming film \"The Batman\" (2022).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is the Virat Kohli ?\"\n",
        "response = FINAL(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCI6l2Nwuebs",
        "outputId": "457e1fb5-1471-4932-909c-7c1b8142edc5"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is outside the topic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Which is the highest IMDB rating batman movie till date?\"\n",
        "response = FINAL(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "7_OGjjPiu5K_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d198fb-3ebb-4cf3-e282-dd0e32d365ca"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Dark Knight is the highest IMDB rated Batman movie till date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who won 2024 elections?\"\n",
        "response = FINAL(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJAPQPMoFkAh",
        "outputId": "0909f20f-8564-404f-88e3-8f6d4d38445b"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is outside the topic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Highest imdb movie rated till date?\"\n",
        "response = FINAL(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI6L7Lx_G2Cq",
        "outputId": "6b1d3b7a-b0d0-4e95-a55c-1f30f52623f5"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is outside the topic. I'm not able to provide information on the highest IMDb movie rated till date as it's beyond my expertise.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EvsC838lRfDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}